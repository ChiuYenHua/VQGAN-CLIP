{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLASS-VQGAN+CLIP.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkUfzT60ZZ9q"
      },
      "source": [
        "#@markdown #**Check GPU**\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSfISAhyPmyp",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Install module**\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning einops transformers\n",
        "!pip install -e ./taming-transformers\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhhdWrSxQhwg",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Curl VQGAN**\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' > vqgan_imagenet_f16_1024.yaml\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1' > vqgan_imagenet_f16_1024.ckpt\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' > vqgan_imagenet_f16_16384.yaml\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' > vqgan_imagenet_f16_16384.ckpt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #**One Big Class**\n",
        "import argparse\n",
        "import io\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "import requests\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from CLIP import clip\n",
        "\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
        "        vals = prompt.rsplit(':', 3)\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        return clamp_with_grad(torch.cat(cutouts, dim=0), 0, 1)\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "#------------Main Function--------------#\n",
        "\n",
        "# Generate Picture\n",
        "def generate_pic(args,first):\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  print('Using device:', device)\n",
        "\n",
        "  model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "  perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "\n",
        "\n",
        "  cut_size = perceptor.visual.input_resolution\n",
        "  e_dim = model.quantize.e_dim\n",
        "  f = 2**(model.decoder.num_resolutions - 1)\n",
        "  make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "  n_toks = model.quantize.n_e\n",
        "  toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "  sideX, sideY = toksX * f, toksY * f\n",
        "  z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "  z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "  if args.seed is not None:\n",
        "      torch.manual_seed(args.seed)\n",
        "\n",
        "  if args.init_image:\n",
        "      pil_image = Image.open(fetch(args.init_image)).convert('RGB')\n",
        "      pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n",
        "      z, *_ = model.encode(TF.to_tensor(pil_image).to(device).unsqueeze(0) * 2 - 1)\n",
        "  else:\n",
        "      one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "      z = one_hot @ model.quantize.embedding.weight\n",
        "      z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "  z_orig = z.clone()\n",
        "  z.requires_grad_(True)\n",
        "  opt = optim.Adam([z], lr=args.step_size)\n",
        "\n",
        "  normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                  std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "  pMs = []\n",
        "\n",
        "  for prompt in args.prompts:\n",
        "      txt, weight, stop = parse_prompt(prompt)\n",
        "      embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "      pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "  for prompt in args.image_prompts:\n",
        "      path, weight, stop = parse_prompt(prompt)\n",
        "      img = resize_image(Image.open(fetch(path)).convert('RGB'), (sideX, sideY))\n",
        "      batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "      embed = perceptor.encode_image(normalize(batch)).float()\n",
        "      pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "  for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "      gen = torch.Generator().manual_seed(seed)\n",
        "      embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "      pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "  def synth(z):\n",
        "      z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "      return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def checkin(i, losses):\n",
        "      losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "      tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "      out = synth(z)\n",
        "      # Save pic #################\n",
        "      pic_name = str(first) + 'folder/' + str(i) + '_' + 'pic.png'\n",
        "      TF.to_pil_image(out[0].cpu()).save(pic_name)\n",
        "      print(pic_name)\n",
        "      display.display(display.Image(pic_name))\n",
        "\n",
        "  def ascend_txt():\n",
        "      out = synth(z)\n",
        "      #######################\n",
        "      iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
        "\n",
        "      result = []\n",
        "\n",
        "      if args.init_weight:\n",
        "          result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
        "\n",
        "      for prompt in pMs:\n",
        "          result.append(prompt(iii))\n",
        "\n",
        "      return result\n",
        "\n",
        "  def train(i):\n",
        "      opt.zero_grad()\n",
        "      lossAll = ascend_txt()\n",
        "      if i % args.display_freq == 0:\n",
        "          checkin(i, lossAll)\n",
        "      loss = sum(lossAll)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      with torch.no_grad():\n",
        "          z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "\n",
        "  i = 0\n",
        "  stop_iteration =  510#@param {type:\"number\"}\n",
        "  try:\n",
        "      with tqdm() as pbar:\n",
        "          while True:\n",
        "              if i==stop_iteration :\n",
        "                break\n",
        "              train(i)\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "  except KeyboardInterrupt:\n",
        "      pass\n",
        "\n",
        "# Predict Noun\n",
        "def predict_noun(image):\n",
        "  import os\n",
        "  import clip\n",
        "  import torch\n",
        "  from torchvision.datasets import CIFAR100\n",
        "\n",
        "  # Load the model\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "  # Download the dataset\n",
        "  cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
        "\n",
        "  # Prepare the inputs\n",
        "  image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "  text_inputs = torch.cat([clip.tokenize(f\"a picture of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "  # Calculate features\n",
        "  with torch.no_grad():\n",
        "      image_features = model.encode_image(image_input)\n",
        "      text_features = model.encode_text(text_inputs)\n",
        "\n",
        "  # Pick the top 5 most similar labels for the image\n",
        "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "  similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "  values, indices = similarity[0].topk(5)\n",
        "\n",
        "\n",
        "  # Print the result\n",
        "  print(\"\\nTop predictions:\\n\")\n",
        "  for value, index in zip(values, indices):\n",
        "      print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")\n",
        "\n",
        "# Generage_video\n",
        "def generate_video(freq,first):\n",
        "  import os\n",
        "  import numpy as np\n",
        "\n",
        "  frames = os.listdir('/content/'+str(first)+'folder')\n",
        "  frames = len(list(filter(lambda filename: filename.endswith(\".png\"), frames))) #Get number of jpg generated\n",
        "\n",
        "  init_frame = 0 #This is the frame where the video will start\n",
        "  last_frame = frames #You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
        "\n",
        "  min_fps = 10\n",
        "  max_fps = 30\n",
        "\n",
        "  total_frames = last_frame-init_frame\n",
        "\n",
        "  #Desired video time in seconds\n",
        "  video_length = 20\n",
        "\n",
        "  frames = []\n",
        "  tqdm.write('Generating video...')\n",
        "  for i in range(init_frame,last_frame): #\n",
        "      filename = '/content/'+str(first)+f'folder/{i*freq}_pic.png'\n",
        "      frames.append(Image.open(filename))\n",
        "\n",
        "  fps = np.clip(total_frames/video_length,min_fps,max_fps)\n",
        "\n",
        "  from subprocess import Popen, PIPE\n",
        "  p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', 'video.mp4'], stdin=PIPE)\n",
        "  for im in tqdm(frames):\n",
        "      im.save(p.stdin, 'PNG')\n",
        "  p.stdin.close()\n",
        "\n",
        "  print(\"The video is now being compressed, wait...\")\n",
        "  p.wait()\n",
        "  print(\"The video is ready\")\n",
        "\n",
        "  # Download\n",
        "  from google.colab import files\n",
        "  files.download(\"video.mp4\")\n",
        "\n",
        "  # Rename To move To folder\n",
        "  os.rename(\"/content/video.mp4\", '/content/'+str(first)+'folder/'+'video.mp4')\n",
        "\n",
        "# Class AI_painter\n",
        "class AI_painter:\n",
        "  def paint(self):\n",
        "    pic1 = 'https://raw.githubusercontent.com/ChiuYenHua/style-clip-draw/master/paintings/01picasso1-superJumbo.jpg'  #@param {type:\"string\"}\n",
        "    pic2 = 'https://raw.githubusercontent.com/ChiuYenHua/style-clip-draw/master/paintings/2021_HGK_20415_0001_000(jean-michel_basquiat_warrior090115).jpg'#@param {type:\"string\"}\n",
        "    pic3 = 'https://raw.githubusercontent.com/ChiuYenHua/style-clip-draw/master/paintings/Claude-Monet-Waterlilies-and-Japanese-Bridge-1899_HIGH-RES.jpg'#@param {type:\"string\"}\n",
        "    pic4 = 'https://raw.githubusercontent.com/ChiuYenHua/style-clip-draw/master/paintings/GettyImages-1151386026.jpg'#@param {type:\"string\"}\n",
        "    pic5 = 'https://raw.githubusercontent.com/ChiuYenHua/style-clip-draw/master/paintings/The-Starry-Night.jpg'#@param {type:\"string\"}\n",
        "    pic6 = 'https://raw.githubusercontent.com/ChiuYenHua/style-clip-draw/master/paintings/Botticelli_-_Portrait_of_a_young_man_holding_a_medallion.jpg'#@param {type:\"string\"}\n",
        "    pic7 = 'https://raw.githubusercontent.com/ChiuYenHua/style-clip-draw/master/paintings/Peter_Paul_Rubens_-_The_Feast_of_Venus_-_Google_Art_Project.jpg'#@param {type:\"string\"}\n",
        "    pic8 = 'https://raw.githubusercontent.com/ChiuYenHua/style-clip-draw/master/paintings/pablo-picasso-painting.jpg'#@param {type:\"string\"}\n",
        "    pic9 = 'https://raw.githubusercontent.com/ChiuYenHua/style-clip-draw/master/paintings/10067%20Lot%208%20-%20Claude%20Monet%2C%20Meules.jpg'#@param {type:\"string\"}\n",
        "    picture_list = [pic1,pic2,pic3,pic4,pic5,pic6,pic7,pic8,pic9]\n",
        "    noun = []\n",
        "\n",
        "    # clip predict pic noun\n",
        "    for path in picture_list:\n",
        "      img = resize_image(Image.open(fetch(path)).convert('RGB'), (500, 500))\n",
        "\n",
        "      imgplot = plt.imshow(img)\n",
        "      plt.show()\n",
        "\n",
        "      predict_noun(img)\n",
        "      #noun.append(predict_noun_2343(img))\n",
        "\n",
        "    # transform list to string\n",
        "    sentence = ''.join(noun)\n",
        "    sentence += ' in ghibli studios'\n",
        "\n",
        "    picture_size = 500 #@param {type:\"number\"}\n",
        "    display_frequency =  10#@param {type:\"number\"}\n",
        "    sentenceToGenerate = 'simple' #@param {type:\"string\"}\n",
        "\n",
        "    for pic,first_name in zip(picture_list,range(len(picture_list))):\n",
        "        # setting\n",
        "        args = argparse.Namespace(\n",
        "            prompts=[sentenceToGenerate],\n",
        "            image_prompts=[pic],\n",
        "            noise_prompt_seeds=[],\n",
        "            noise_prompt_weights=[],\n",
        "            size=[picture_size, picture_size],\n",
        "            init_image=None,\n",
        "            init_weight=0.,\n",
        "            clip_model='ViT-B/32',\n",
        "            vqgan_config='vqgan_imagenet_f16_1024.yaml',\n",
        "            vqgan_checkpoint='vqgan_imagenet_f16_1024.ckpt',\n",
        "            step_size=0.05,\n",
        "            cutn=64,\n",
        "            cut_pow=1.,\n",
        "            display_freq=display_frequency,\n",
        "            seed=0,\n",
        "        )\n",
        "\n",
        "        # create folder\n",
        "        import os\n",
        "        os.mkdir('/content/'+str(first_name)+'folder')\n",
        "\n",
        "        # main_function\n",
        "        generate_pic(args,first_name)\n",
        "\n",
        "        # generate video\n",
        "        generate_video(display_frequency,first_name)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ROEKTyBH-bm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = AI_painter()\n",
        "test.paint()"
      ],
      "metadata": {
        "id": "EmGCCVnM-ehb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Pa5zGbiU_4fE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}